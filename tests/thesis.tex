\documentclass[11pt,twocolumn]{jarticle} %11pt が MS-Word の10.5pt 相当
\usepackage[a4paper,left=23mm,right=23mm,top=27mm,bottom=32mm]{geometry} 

\usepackage[dvipdfmx]{graphicx}
\usepackage{iit-en-sjis} %use iit-en-sjis if the body text is written English.
\usepackage{wrapfig}
\usepackage{algorithm, algpseudocode}
\usepackage{algorithmicx}
\usepackage{comment}
\usepackage{biblatex}
\usepackage{amsfonts}
\addbibresource{references.bib} 

\graphicspath{ {imgs/} }

\title{捕食者・獲物環境におけるマルチエージェントの分散型協調学習}
\etitle{Distributed Multi-Agent Cooperation Learning in Predator-Prey Environment}

\author{唐 \ \ 霄}
\eauthor{TANG Xiao}

\advisors{
\footnotesize
\begin{tabular}{ll}
 指導教員: & 延原 肇， 中内 靖， 星野 准一（知能機能工学域）\\
\end{tabular}
}
\eadvisors{
\scriptsize
\begin{tabular}{ll}
Supervised by: & Nobuhara Hajime, Yasushi Nakauchi and Junichi Hoshino (Division of Intelligent Interaction Technologies) \\
\end{tabular}
}

\authorheader{唐 \ 霄}
% use English name when you use the English style

\dateheader{2018年3月}

\abstract{
Research of Moving Target Search is ongoing recent years. There are some researches according to the problem of Multi-Agent pursuing a moving target, but few of them could be applied to real world. This research focused on this problem and proposed a speed-up method for real-time grid environment based on Cover Heuristic method. We use Map Abstraction to build map hierarchy which helps us do faster map search compared to previous method. Refinement is used for highly-abstracted route refining to successor original map. Finally, evaluation experiments are based on Benchmark maps and the result showed high efficiency of our proposed method.
}

\keywords{keyword-1, keyword-2, keyword-3, keyword-4, keyword-5, keyword-6}

\begin{document}

\maketitle
\thispagestyle{iitheader}
\section{Introduction}
[AI Boom] \par
Recently, AI arouse hot topics around the world, especially the appearence of AlphaGo \cite{1}. DeepMind introduced their Go player which is called AlphaGo in 2015, they beat several human being's top professional players of over the past two years. And AlphaGo evolved to higher version? which are AlphaGo Master and AlphaGo Zero \cite{2}. Not only the application to traditional sports, Game is also a big area which study are ongoing. DeepMind and Blizzard released StarCraft II as an AI research environment [reference] to researchers around the world which could help human-being to have a better acknowledge of game strategy.\par

[Application and shortcomings] \par
Deep Reinforcement Learning (DRL) is one of the technologies which support AI development. There are a lot of applications from game playing [reference] to robot controlling [reference]. Also, Google applied Deep Learning to data center coolinng by 40\% \cite{3} electric cost off [reference]. Healthcare and finance are the areas which are being researched and expected to have a greate impact to sociaty. However, despite the fact that DRL is successfully applied to many single-agent domain tasks, there are varianty of applications which are in multi-agent domain. These application needs multiple agents to evolve together resulting in good communication or cooperation. For instance, multi-character controlling in game playing, multi-agent system in delivery system and so on.

[Task]
\begin{figure}[t]
 \begin{center}
  \includegraphics[width=6cm]{imgs/adversary_chasing.png}
  \caption{Adversary Chasing}\label{fig:adversaryChasing}
 \end{center}
\end{figure}

One representative for multi-agent task is Predator-Prey \cite{4}, showed in figure [fig:adversaryChasing]. In above case, there are 
3 predators, 1 prey and 2 landmarks (obstacles) in this map. Predators move with slower speed compared to prey to chase the faster moving prey. For human being, the cooperation strategy of splitting up and surrounding is absolutely easy to understand and learn. Unfortunately, it is difficult for agent to learn. Although Traditional reinforcement learning such as Q-learning \cite{5}, Policy Gradient \cite{6} performs greatly or even over human being in Atari Game \cite{7}, it performs poorly in multi-agent domain. The reason why the successful RL methods using in single-agent domains could not acquire the same result in multi-agent domain is that along with mult-agent self-learning, the environment becomes non-stationary which failed in convergence. \par

[Agenda] \par
In this work, we first introduce several prior work and related researches and why they failed in multi-agent domain. Then we will explain our proposed method - Distributed Multi-Agent Cooperation Algorithm besed on MADDPG algorithm \cite{4}. Our method includes the multiple workers distributly working on each thread and the learning rate tuning using Proximal Policy Optimization \cite{8}. \par

Evaluation experiments shows our method performed better than MADDPG and traditional Deep Reinforcement Learning method DDPG. \par

\section{Background} 
In this section, we introduce several related researches and discuss why they cannot be applied to multi-agent domain task.
\subsection{Cover-hueristic Algorithm}
As for prior research for solving Predator-Prey, I have proposed a cooperation searching algorithm  \cite{9} towards this task. This method is based on map search using cover-heuristic (maxmizing Predator's moving area and minimizing prey's moving area) and accelarate search by map abstraction and refinement. However, this method performs well in small-size maps but poorly in big-size maps, the computational time is depends on the map size. A agent which could be called intelligent should have its own mind like human beings. This kind of AI is able to take actions based one its own policy.\par


\subsection{Reinforcement Learning and Multi-Agent Markov Decision Process}

Reinforcement Learning (RL) is method that agent learns to make decisions through interaction with the environment. An agent interacts with environment and becomes able to alter its behaviour according to the reward it recieved from environment along with observing the consequences of its actions. \par

In RL settings, agent is controlled by specific algorithm (machine learning). At timestep $t$, the agent observes a state $s_t$ from environment and interacts with it by taking an action $a_t$. After agent takes an action, the environment and agent transition to a new state $s_{t+1}$. The state represents sufficient necessary information which could have agent's informations (position, velocity) and other environment information. Each time the environment transitions to a new state, agent recieves an reward $r_t$ as feedback. The goal of the agent is to learn a policy $\pi$ which maximizes the expected cumulative reward. However, the challenge in RL is that agent needs to learn about the consequences of actions by trail and error instead of one step action. The action-learning loop is illustrated in Figure \ref{fig:rl}. \par
\begin{figure}[h]
 \begin{center}
  \includegraphics[width=8cm]{imgs/RL.PNG}
  \caption{
  The Agent-Action-Learning loop. At time $t$, the agent recieves state $s_t$ from the environment. The agent uses its policy $\pi$ to choose an action $a_t$. Once the action is executed, the environment transitions a step, providing the next state $s_{t+1}$ as well as feedback in the form of a reward $r_{t+1}$. The agent uses knowledge of state transitions, of the form ($s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$), in order to learn and improve its policy.
  }
  \label{fig:rl}
 \end{center}
\end{figure}

The RL could be described as a Markov Decision Process (MDP). in our work, we extend MDP from single agent domain to multi-agent setting. The multi-agent MDP consists of (timestep $t$ is omitted):
\begin{itemize}
  \item $n$ Agents.
  \item State $s$ which describes agents' configuration.
  \item A set of observations $o_1$, $o_2$ ... $o_i$ ... $o_n$ from state, $_i$ is agent's index.
  \item A set of actions $a_1$, $a_2$ ... $a_i$ ... $a_n$ chosen from each agent.
  \item Policy $\pi_i$ of agent $i$: \[\pi_i: o_i \rightarrow a_i\] which represents given the observation, agent uses its policy to choose an action.
  \item A set of rewards $r_1$, $r_2$ ... $r_i$ ... $r_n$, agents recieves rewards from environment after execution of their chosen actions.
\end{itemize}
Each agent is trying to maxmize its total expected reward, which could be represents as,
\[ \sum_{t=0}^{T-1}(\gamma^t r_i^{t+1}) \]
where $t$ is timestep and $T$ is the length of one episode, gamma is the discounted factor $\gamma$ $\subseteq$ $(0, 1]$

\section{Related Work}

Currently, there are three approaches to solve MDP problems: value-based method, policy-based method and actor-critic method which combines value and policy method. We will discuss these methods about their advantages and also shortcomings in solve multi-agent domain problems.

\subsection{Value-based Method}

Value-based method is based on estimating the expected return of being given a state. The state-value function $V^\pi(s) = \mathbb{E}[R|s, \pi]$ is expected return when starts from state $s$ and following the policy $\pi$. When we have optimal policy, $\pi$ becomes $\pi^*$. \par
However, value function can not provide us transitions for agent to learn. Therefore, action-value function $Q^\pi(s, a)$ could help. Action-value function needs initial action $a$ input and state $s$, the following state is based on policy $\pi$. The best policy could be found by choosing maximal $Q$ value with action a which represents as $argmax_a Q^\pi(s, a)$. 

\subsubsection{Q-learning}

Q-learning is one of famous algorithms in RL. We update action-value function using a Bellman equation where t is timestep: 
\begin{equation}
Q^\pi(s_t, a_t) = \mathbb{E}_{s+1}[r_{t+1} + \gamma Q^\pi(s_{t+1}, \pi(s_{t+1}))].  
\end{equation}
\begin{equation}
Q^\pi(s_t, a_t) \leftarrow Q^\pi(s_t, a_t) + \alpha\delta.  
\end{equation}
where $\alpha$ is learning rate and $\delta = y - Q^\pi(s_t, a_t)$ is the Temporal Difference (TD) error; We can see that $Q^\pi$ can be improved by \textsl{bootstrapping}.\par 
Q-learning is an off-policy algorithm, because $Q^\pi$ is updated by transitions that were not generated by derived policy where $y = r_t + \gamma\max_a Q^\pi(s_{t+1}, a)$which is the approximation of $Q^*$.
\subsubsection{Deep Q-learning}

Deep Q-learning (DQN) \cite{10} is the extended version of Q-learning using deep learning. It uses a deep neural network to work as the Q value function. 
It has a much better performance compared with original RL methods. Especially in discrete action space, DQN reached nearly human level in Atari games.
\begin{equation}
L(\theta) = E_{s,a,r,s'}[(Q^*(s, a|\theta) - y)^2],  
\end{equation}
$$where\ y = r + \gamma\max \bar{Q}^*(s', a')$$
where $\theta$ is the parameters in neural network, $(s,a,r,s')$ is the transition we used. \par
The TD-loss could be presented as blow, using current value function and the difference among batch data. Then it uses Stocastic Gradient Desecent to minimize the loss, resulting in convergence of value function. Also, the target network and replay memory helps to fix the non-stationary problem of learning. \par
However, DQN can only handle discrete and low-dimentional action spaces. It cannot be directly applied to continuous domains because it relies on find actions which could maxmize the value function. Value-iteration to find the optimal action at every time step is almostly impossible in continous action spaces. In multi-agent predator-prey environment, it is not the direction the agent dicides to go, but the power in each direction and considering the acceleration, frictional force， which resulting in a natural moving.

\subsection{Policy-based Method}

\subsection{Actor-Critic Method}

\subsection*{Deep Deterministic Policy Gradient}
DQN made great success in high-dimensional observation spaces and discrete action space, but failed in continous action spaces. Deep Deterministic Policy Gradient (DDPG) \cite{6} is based on Deterministic Policy Gradient \cite{11} and could solve high-dimensional continuous action spaces tasks. DDPG is an actor-critic method, actor represents agent's policy $\pi$ and critic is to evaluate the action which policy would take using Value function like DQN. 
\begin{equation}
L(\theta) = E_{s,a,r,s'}[(Q^*(s, a|\theta) - y)^2] 
\end{equation}
\begin{equation}
J(\mu) = E[Q^\mu(s, a) | _{a=\mu(s)}]
\end{equation}
During training time, actor trys to maxmize $J(\mu)$ using Stocastic Gradient Acsent and critic trys to minimize TD-loss using SGD. DDPG also applied target network and replay memory used in DQN. \par
However, policy of each agent is changing during training proccess, and the environment becomes non-stationary due to the fact agent could not predict next state with its own policy. This issue would prevent training stability and the use of experience replay memory. DDPG could not solve multi-agent setting problems.

\subsection*{Advantage Actor-Critic}
Different from DQN or DDPGs, another method using Actor-critic architecture uses advantage for learning. 
There are Asychronous \cite{12} and Sychronous \cite{13} version of Advantage method.\par
$A(s_t, a_t; \theta, \theta_v) = $ 
\begin{equation}
\sum_{i=0}^{k-1}(\gamma^i r_{t+i} + \gamma^k V(s_{t+k};\theta_v)) - V(s_t; \theta_v).
\end{equation}
One point which A2C or A3C outperforms over traditional methods is Multi-Worker idea. Actor-Critic learner runs on each separate thread and updates experience or gradient to global actor-critic. Global actor-critic gets data from workers sync or async, and tries to learn optimal policy. This multi-worker idea will be adopted in our method.

[Please stop here]
\section{Proposed Methods}[still need more time on section 3]
We have discussed in previous sections that traditional reinforcement learning methods perform poorly in our multi-agent domain environment. In this section, we will introduce our proposed method: Distributed Multi-Agent Cooperation Learning. The method could be split into two part. One is the Multi-Worker using threading programming. And another one is the method we use for parameter tuning.
[Some constrains or obvious advantage of our proposed method]

In traditional DRL methods like DQN or DDPG, we use replay memory to solve the problem that there are close connections between transitions. When training, add transition into repley memory, then when updating network, sample batches from replay memory with a fixed size (64 or 128). However, along with the network updating, old experiences seems less meaningful for learning comparing with batches which were gotten from recent policy. 
In our Distributed method, we use several (2....n) workers to work for us, in charge of collecting batch data. Each worker runs on different threading with same environment (different random noise for each worker), collection (state, actions, rewards, done, next state) then pushing into a fixed-size memory (64 or 128). When memory is full, our brain starts to update using this batch data. Because we have different work working on parallel world, connection problem between batch data is solved in our method.  

\subsection{Centralized Critic and decentralized Actor}
[MADDPG]
Recently, OpenAI rleased a method which extends traditional DDPG method to multi-agent domain \cite{4}. As we know, single-agent algorithm failed because while agent is updating policy, the environment becomes non-stationary which turns out to failure of convergence. \textbf{Multi-Agent DDPG} found a centralized way to put other agents's actions into consideration in critic.
\begin{equation}
L(\theta_i) = E_{s,a,r,s'}[(Q^*(s, a_1, a_2 ... a_n) - y)^2],  
\end{equation}
$$where\ y = r_i + \gamma{Q_i}^*(s', a_1', a_2' ... a_n') | _{a_j'=\mu_j'(o_j)}$$
This is a great iead to learn using centralized critic and act using decentralized actor. 

Based on the centralized critic idea from MADDPG, we introduce our two approaches towards multi-agent task in Predator-Prey environment.


\subsection{Distributed Method based on action-value function as appro}
As we mentioned in 2.6, we got inspiration from MADDPG which is a multi-agent extension of DDPG. we could extend this centralized thought to critic using advantage (TD-error).
\begin{equation}
A(s_t, a_t; \theta, \theta_v) = \sum_{i=0}^{k-1}(\gamma^i r_{t+i} + \gamma^k V(s_{t+k};\theta_v)) - V(s_t; \theta_v).
\end{equation}
\subsection{Adjust learning using Proximal Policy Optimization}
When we are calculating Advantage which could be also called TD-error, by clipping the advantage, we could adjust a proper learning rate for updating actor and critic. This clipping idea comes from clipping suggorate objective in PPO paper. By getting ratio between policy and new policy, we could get the minimal Advantage of ratio and (1-epison, 1+epison), we choose epison = 0.2

\subsection{Proximal Policy Optimization}
Althoughm Policy gradient methods are foundamental break-through using Deep Neural Network, it is difficult to obstain good result using policy gradient methods because it is extremely sensetive about choosing learning step size. If iteration step size is too small, training will be very slow; if it is too large, training process will be influenced by noise which turns out to a poor performance and over-fitted result. \par
Proximal Policy Optimization is the method which gets inspiration from Trust Region Policy Optimization and does a well trade-off among implementation simplicity, Sample complexity and Tuning difficulty. PPO tries to calculate a update in every iteration step to minimize lost function and ensures a small deviation compared with previous policy when calculating gradient.

\subsection{Distributed Multi-Agent Cooperation Architecture}
\begin{figure*}[t]
 \begin{center}
  \includegraphics[width=12cm]{imgs/architecture.PNG}
  \caption{architecture}
  \label{fig:architecture}
 \end{center}
\end{figure*}

There is brain and several workers in our architecture. Fu We use several workers to work in parallel world to collect batch data with low connectivity. In each worker,  we use brain's policy to choose action, then we give this action (using some noise) to environment and get the batch data (state, actions, rewards, done, next state). And we store this data in to a exchange memory which could accessed by workers (working in each thread). When we have stored batch up to a predifined size, workers stop collecting data from each environment. Meantime, our brain starts to update network using batch data.

\begin{algorithm*}
\caption{Distributed Worker on each thread Algorithm}
\begin{algorithmic}
\State {Initialize a random process $\mathbb{N}$ for action exploration}
\State {Receive initial state $s$}
\For {$t = 1$ to max-episode-length}
  \State {for each agent $i$, select action $a_i = \pi_i(o_i) + \mathbb{N}_t$ w.r.t the current policy and exploration}
  \State {Execute actions $a = (a_1 ... a_n)$ and observe reward $r = (r_1 ... r_n)$ for each agent and new state $s'$}
  \If {Queue size is not equal to max-batch-size}
    \State {Store $(s, a, r, s')$ in Queue which could be accessed by other thread}
  \EndIf
\EndFor
\end{algorithmic}
\end{algorithm*}

\begin{algorithm*}
\caption{Distributed Multi-Agent Cooperation Algorithm based action-value function}
\begin{algorithmic}
\State $Initiate brain and n workers for n agents$
\State $$
\For {$episode = 1 to M$}
  \State {$$}
  \For {$t = 1$ to max-episode-length}
    \State {Get batch data $(s, a, r, s')$ from Queue}
  \EndFor
\EndFor
\While {$queue \neq empty$}
    \State $node1 \gets queue.pop()$
    \State {$node1\ mark\ as\ "abstracted"$}
    \If {node1 has no unabstracted neighbor}
        \State {Abstract node1 as (level+1) node to new Graph with node1's position}
        \State {$Continue$}
    \EndIf
    \State {$node2 \gets node1's\ unabstracted\ neighbor$}
    \State {Abstract node1, node2 as (level+1) node to new Graph with node1 and node2's average position}
    \State {Push node1, node2's unabstracted neighbor to queue}
\EndWhile
\Return {$new graph$}
\end{algorithmic}
\end{algorithm*}

\section{Empiral Experiments}
\subsection{Experiment Environment}

\subsection{Network Architecure}
\begin{figure}[h]
 \begin{center}
  \includegraphics[width=8cm]{imgs/actor.PNG}
  \caption{Actor Network Architecture}\label{fig:Actor}
 \end{center}
\end{figure}

\begin{figure}[h]
 \begin{center}
  \includegraphics[width=8cm]{imgs/critic.PNG}
  \caption{Critic Network Architecture}\label{fig:Critic}
 \end{center}
\end{figure}

\begin{comment}

\section{序論}
執筆要領について述べる。

\subsection{用紙}
A4用紙を縦長に使用する。本文は8ページ（表裏で4枚）以上、24ページ以内を標準とする。

\subsection{ヘッダ・フッタ}
ヘッダの左側には、「筑波大学大学院博士課程システム情報工学研究科修士論文」につづけて、修了年度・月を括弧書きで記述する。
ヘッダの右側には、氏名を記述する。氏名については、見やすくするために適宜空白をいれても差し支えない。

フッタの中央には、ページ番号をハイフンはさんで記述する。

\subsection{1ページ目の体裁}
申請する学位（修士（工学））を四角囲みで記述し、
つづけて修士論文のタイトルをそれぞれ日英表記で記述する。
ただし本文が英文の場合、英日の順に記述する。

つぎに、氏名・所属専攻と指導教員名・指導教員の所属を日英表記で記述する。
氏名・指導教員名については、見やすくするために適宜空白をいれても差し支えない。
ただし本文が英文の場合、英日の順に記述する。


\subsection{要約・キーワード}
修士論文の概要を記述する。
本文が日本語・英語に係らず概要は英語で記述する。


\subsection{本文}
本文は2段組みとし、10.5pで記述する。
句読点は、各分野で用いられる記号を使用する。

\subsection{図表}
論文として印刷に耐えうる品質（解像度）の図表を作成する。
図表中、および、キャプションは、原則として英語を使用する。
キャプションは、図の下と表の上に挿入する。
図番号および表番号は、それぞれ Figure 1、Table 1 のように表記する。
横長の図・表の場合は、段抜きで挿入してもよい。

たとえば、Figure \ref{fig:samplefigure}や Table \ref{tbl:sampletable}を例として示す。

\begin{figure}[t]
 \begin{center}
  \includegraphics{sample.eps}
  \caption{Sample figure}\label{fig:samplefigure}
 \end{center}
\end{figure}

\begin{table}
 \caption{Sample table} \label{tbl:sampletable}
\begin{center}
  \begin{tabular}{c|ccc}
  A & B & C & D\\
  \hline \hline
  a & b & c & d\\
  a & b & c & d\\ \hline
 \end{tabular}
\end{center}
\end{table}

\subsection{数式}
数式のサンプルとして、(\ref{eq:gauss})を示す。
\begin{equation}
 \int _\infty ^\infty e^{-x^2}dx = \sqrt{\pi} \label{eq:gauss}
\end{equation}

\subsection{参考文献}
それぞれの分野の記述法により、十分な数の参考文献を引用する\cite{osamu}。著者が執筆した修士論文に関連する内容の論文等がある場合には、必ず文献として引用する。

\subsection{著者紹介}
学会誌論文の一般的な著者紹介を例に記述する。

\subsection{付録}
本文には書ききれないデータなどは、
付録に収めることができる。付録は、研究室で別途保管する。



\section{\LaTeX スタイルファイルについて}
このサンプルは、筑波大学大学院システム情報工学研究科知能機能システム専攻の修士論文を\LaTeX で作成するためのスタイルファイルである。
配布ファイルは以下の通りである。
\begin{description}
 \item[iit-jp-sjis.sty]  知能機能システム専攻の修士論文のためのスタイルファイル
 \item[templete.tex] スタイルファイルを利用するためのテンプレートファイル
 \item[face.eps] 顔写真用のテスト画像
 \item[sample.eps] Figure のためのテスト画像
\end{description}

スタイルファイルやテンプレートは、適宜改良をして使用してもよい。

\section*{謝辞}
本研究は、・・・・・・深謝する。


%\addcontentsline{toc}{section}{\numberline{}参考文献}

\begin{thebibliography}{9}
\bibitem{osamu}  工学修得, ``修士論文の書き方'', 知能機能システム学会論文誌, Vol.1, No.2, pp.34--56, 2015.
\end{thebibliography}


\vspace{2zh}
\begin{minipage}{73mm}
 \begin{wrapfigure}[6]{l}[-4pt]{30mm} 
 \begin{center}
  \includegraphics[width=30mm]{face.eps}
 \end{center}
 \end{wrapfigure}
 \noindent 筑 \ 波 \ 太 \  郎\\\\
 筑波大学大学院システム情報工学研究科知能機能システム専攻所属
\end{minipage}

\vspace{3zh}
----------------------------------------------\par

\begin{itemize}
 \item Webで公開する論文の概要は、別に作成する。
 \item 修士論文は両面印刷とする。
 \item 修士論文提出時は、ソフトカバーを施して必要部数を大学院教務に提出する。背表紙は不要。
 \item 論文審査後に、両面印刷したもの（綴じていないもの）を専攻長に提出する。
 \item 専攻長は、専攻全員の修士論文をハードカバーにて製本し、専攻室で保管する。
\end{itemize}

\begin{itemize}
 \item Extended summary is also required for uploading onto the web.
 \item The thesis should be printed in double face printing.
 \item Submit your thesis to the academic service office. The thesis should be clapped with a soft-cover.
 \item  Submit your thesis to the chair of the iit, after it has passed the examining meeting. The thesis should not be clapped.
 \item The chair of the iit will bind up all the accepted theses and preserve in his/her office.
\end{itemize}
https://scholar.google.com/scholar?hl=zh-CN&as_sdt=02C5&q=policy+gradient&btnG=
\end{comment}

\begin{thebibliography}{9}
\bibitem{1}  工学修得, ''修士論文の書き方'', 知能機能システム学会論文誌, Vol.1, No.2, pp.34--56, 2015.
\bibitem{2} "https://arxiv.org/abs/1712.01815"
\bibitem{3} "https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/"
\bibitem{4} "https://arxiv.org/abs/1706.02275"
\bibitem{5} "https://link.springer.com/article/10.1007/BF00992698"
\bibitem{6} "xxx"
\bibitem{7} "https://arxiv.org/abs/1509.02971"
\bibitem{8} "https://arxiv.org/abs/1707.06347"
\bibitem{9} "リアルタイムグリッド環境における単一移動目標捕獲マルチエージェントの性能向上"
\bibitem{10} "https://www.nature.com/articles/nature14236"
\bibitem{11} "http://proceedings.mlr.press/v32/silver14.pdf"
\bibitem{12} "http://proceedings.mlr.press/v48/mniha16.pdf"
\bibitem{13} " https://blog.openai.com/baselines-acktr-a2c/"
\end{thebibliography}

\end{document}
