monte-carlo 
V(St) <- V(St) + alpha(Gt - V(St))

Return Gt = Rt+1 + gamma*Rt+2 .... gamma^T-1 RT; higher variance
- depends on many random actions, transitions, rewards

1. learn directly from episodes of experience
2. Model-free: no knowledge of MDP transitions / rewards
3. learn from complete episodes (no bootstrapping)
4. value = mean return

- High variance, zero bias
	- 


Temporal-Difference Learning
V(St) <- V(St) + alpha(Rt+1 + gamma*V(St+1) - V(St))
TD target: Rt+1 + gamma*V(St+1)
TD error = Rt+1 + gamma*V(St+1) - V(St)

1. Directly learn from episodes of experience
2. Model-free
3. learn from incomplete episodes by bootstrapping
4. updates one guess towards one guess

TD target is much lower variance than the return
- depends on one random action, transition, reward

- Low variance, some bias
